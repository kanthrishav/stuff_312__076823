Below is a surgical set of changes you can drop into your existing file without breaking the current structure. The key idea is:

Keep PlacementProblem.compute_metrics(x) exactly as your “base terms” provider (objective terms + constraint measures).

Add a Formulation Engine that decides how to combine those terms (or how to solve a constrained formulation via a solver).

Route every optimizer’s objective call through one single function:
manager.formulation.evaluate_scalar(decision_vector)
(and for discrete encodings it decodes to coordinates internally).

I’m not regenerating the whole code; I’m giving new functions/classes + exactly where to call them.

1) UI changes: add “Formulation” dropdown + dynamic formulation parameters
Add widgets (near your optimizer widgets)

Add this right after optimizer_select / threads_input area:

# ------------------ Formulation selection (NEW) ------------------

FORMULATION_OPTIONS = [
    "0. Soft penalty (weighted sum)",                       # current
    "1. Barrier constraints (interior-point)",
    "2. Lexicographic (hierarchical) optimization",
    "3. Feasible direction / active-set (SQP-style)",
    "4. Reduced space / manifold mapping",
    "5. Discrete encoding: Sequence Pair",
    "6. Discrete encoding: B*-tree",
    "7. Discrete encoding: Slicing tree",
    "8. MIP/MILP model (mixed-integer)",
    "9. CP/CP-SAT packing",
]

formulation_select = pn.widgets.Select(
    name="Formulation",
    options=FORMULATION_OPTIONS,
    value=FORMULATION_OPTIONS[0],
    width=260,
)

# dynamic formulation params (2 rows like optimizer params)
form_row4 = pn.Row(css_classes=["param-row"])
form_row5 = pn.Row(css_classes=["param-row"])

Add formulation-parameter widgets (only shown depending on selection)

Example set (minimal but “rigorous enough” knobs):

# Barrier params
barrier_type = pn.widgets.Select(
    name="barrier_type",
    options=["log", "reciprocal", "reciprocal_square", "power", "exponential"],
    value="log",
    width=140,
)
barrier_mu = pn.widgets.FloatInput(name="mu", value=1.0, step=0.1, width=80)
barrier_power_p = pn.widgets.FloatInput(name="p", value=2.0, step=0.5, width=70)  # for power barrier
barrier_exp_k = pn.widgets.FloatInput(name="k", value=10.0, step=1.0, width=70)   # for exponential barrier
barrier_line_beta = pn.widgets.FloatInput(name="ls_beta", value=0.5, step=0.05, width=80)
barrier_alpha0 = pn.widgets.FloatInput(name="alpha0", value=1.0, step=0.1, width=70)
barrier_feas_eps = pn.widgets.FloatInput(name="feas_eps", value=1e-6, step=1e-6, width=90)

# Lexicographic params
lex_v_eps = pn.widgets.FloatInput(name="eps", value=1e-4, step=1e-5, width=90)
lex_phase1_maxiter = pn.widgets.IntInput(name="phase1_maxiter", value=200, step=10, width=110)
lex_phase2_solver = pn.widgets.Select(
    name="phase2_solver",
    options=["SLSQP", "trust-constr"],
    value="SLSQP",
    width=120,
)

# Reduced-space params
reduce_map = pn.widgets.Select(
    name="map",
    options=["sigmoid", "tanh"],
    value="sigmoid",
    width=120,
)

# Discrete encodings params
disc_key_bounds = pn.widgets.FloatInput(name="key_range", value=1.0, step=0.5, width=90)
disc_decode_spacing = pn.widgets.FloatInput(name="decode_spacing", value=1.0, step=0.1, width=120)

# MILP/CP params
int_scale = pn.widgets.IntInput(name="int_scale", value=10, step=1, width=90)
time_limit_s = pn.widgets.FloatInput(name="time_limit_s", value=10.0, step=1.0, width=110)
cp_num_workers = pn.widgets.IntInput(name="cp_workers", value=4, step=1, width=90)
big_m = pn.widgets.FloatInput(name="big_M", value=10000.0, step=100.0, width=110)

FORM_PARAM_WIDGETS = {
    "0. Soft penalty (weighted sum)": [],
    "1. Barrier constraints (interior-point)": [
        barrier_type, barrier_mu, barrier_power_p, barrier_exp_k,
        barrier_alpha0, barrier_line_beta, barrier_feas_eps
    ],
    "2. Lexicographic (hierarchical) optimization": [
        lex_v_eps, lex_phase1_maxiter, lex_phase2_solver
    ],
    "3. Feasible direction / active-set (SQP-style)": [
        lex_v_eps,  # reuse as feasibility tol / eps
        lex_phase2_solver,
    ],
    "4. Reduced space / manifold mapping": [reduce_map],
    "5. Discrete encoding: Sequence Pair": [disc_key_bounds, disc_decode_spacing],
    "6. Discrete encoding: B*-tree": [disc_key_bounds, disc_decode_spacing],
    "7. Discrete encoding: Slicing tree": [disc_key_bounds, disc_decode_spacing],
    "8. MIP/MILP model (mixed-integer)": [int_scale, time_limit_s, big_m],
    "9. CP/CP-SAT packing": [int_scale, time_limit_s, cp_num_workers],
}

def _update_formulation_rows(event=None):
    f = formulation_select.value
    widgets = FORM_PARAM_WIDGETS.get(f, [])
    half = (len(widgets) + 1)//2
    form_row4.objects = widgets[:half]
    form_row5.objects = widgets[half:]

formulation_select.param.watch(_update_formulation_rows, "value")
_update_formulation_rows()

Put formulation widgets into the Parameters section layout

In your parameters_left = pn.Column(...) insert one new row + the two dynamic rows:

pn.Row(formulation_select, css_classes=["param-row"]),
form_row4,
form_row5,


Also: in on_run_click, collect formulation selection + params similarly to optimizers.

2) Core change: add a “Formulation Engine” layer
Add these NEW helpers inside PlacementProblem

You need a way to express constraints as generic inequalities g_k(x) > 0 (for barrier + constrained solvers), and as violations (for penalties / lexicographic).

Add:

class PlacementProblem:
    ...

    def objective_terms(self, x: np.ndarray) -> Dict[str, float]:
        m = self.compute_metrics(x)
        return {
            "hpwl": m["hpwl"],
            "bbox_aspect": m["bbox_aspect"],
            "bbox_area": m["bbox_area"],
        }

    def constraint_terms(self, x: np.ndarray) -> Dict[str, float]:
        m = self.compute_metrics(x)
        return {
            "overlap_max_ratio": m["overlap_max_ratio"],
            "spacing_x_violation": m["spacing_x_violation"],
            "spacing_y_violation": m["spacing_y_violation"],
            "bbox_ar_violation": m["bbox_ar_violation"],
            "boundary_violation": m["boundary_violation"],
        }

    def total_violation_scalar(self, x: np.ndarray) -> float:
        """
        Generic scalar V(x) >= 0 for lexicographic:
        Works for future added constraints if you include them in constraint_terms().
        """
        c = self.constraint_terms(x)
        return (
            c["overlap_max_ratio"]**2
            + c["spacing_x_violation"]**2
            + c["spacing_y_violation"]**2
            + c["bbox_ar_violation"]**2
            + c["boundary_violation"]**2
        )

    def inequality_slacks_g(self, x: np.ndarray, smooth_k: float = 10.0) -> List[Tuple[str, float]]:
        """
        Returns list of (name, g_value) where g_value > 0 implies feasible.
        This is what enables:
          - barrier formulations
          - SciPy constrained solvers

        Note: no-overlap and spacing are disjunctive; we provide a smooth approximation:
            g_ij = (1/k) log(exp(k*g_x) + exp(k*g_y))
        which approximates max(g_x, g_y).
        """
        centres = self.decode_centres(x)
        cons = self.constraints_cfg
        min_dx = float(cons.get("min_spacing", {}).get("min_dx", 0.0))
        min_dy = float(cons.get("min_spacing", {}).get("min_dy", 0.0))

        # edges
        edges = []
        for (cx, cy), r in zip(centres, self.rectangles):
            half_w = r.width/2.0
            half_h = r.height/2.0
            edges.append((cx-half_w, cx+half_w, cy-half_h, cy+half_h, r.width, r.height))

        gs: List[Tuple[str, float]] = []

        # Boundary as strict inequalities
        for i, ((cx, cy), r) in enumerate(zip(centres, self.rectangles)):
            half_w = r.width/2.0
            half_h = r.height/2.0
            gs.append((f"bound_left[{i}]",  (cx - half_w) - 0.0))
            gs.append((f"bound_right[{i}]", self.canvas_width - (cx + half_w)))
            gs.append((f"bound_bottom[{i}]", (cy - half_h) - 0.0))
            gs.append((f"bound_top[{i}]",   self.canvas_height - (cy + half_h)))

        # Pairwise separation (smooth disjunction)
        n = self.n_rects
        k = float(smooth_k)
        for i in range(n):
            li, ri, bi, ti, wi, hi = edges[i]
            for j in range(i+1, n):
                lj, rj, bj, tj, wj, hj = edges[j]

                # gaps in x and y
                if ri <= lj:
                    gap_x = lj - ri
                elif rj <= li:
                    gap_x = li - rj
                else:
                    gap_x = 0.0

                if ti <= bj:
                    gap_y = bj - ti
                elif tj <= bi:
                    gap_y = bi - tj
                else:
                    gap_y = 0.0

                g_x = gap_x - min_dx
                g_y = gap_y - min_dy

                # smooth max(g_x, g_y)
                g = (1.0/k) * math.log(math.exp(k*g_x) + math.exp(k*g_y) + 1e-12)
                gs.append((f"sep[{i},{j}]", g))

        # bbox aspect constraint (if enabled)
        cfg = cons.get("bbox_aspect_ratio", {})
        if cfg.get("enabled", False):
            m = self.compute_metrics(x)
            ar = m["bbox_aspect"]
            thr = float(cfg.get("threshold", 1.0))
            typ = cfg.get("type", "less_than")
            if typ == "less_than":
                gs.append(("bbox_ar", thr - ar))    # ar <= thr  => thr-ar >=0
            else:
                gs.append(("bbox_ar", ar - thr))    # ar >= thr  => ar-thr >=0

        return gs


This is the single most important “future-proof” hook: new constraints only need to add:

their violation term into constraint_terms()

and a g_k(x)>0 into inequality_slacks_g() if you want barrier / constrained solvers to use them as “hard”.

3) New Formulation Engine and strategies (options 0–9)

Add a new class (outside PlacementProblem) that:

knows what the current formulation is,

knows if it transforms variables (reduced-space / discrete),

returns scalar objective for continuous optimizers,

or calls solver for MILP/CP-SAT.

FormulationConfig: collect UI selection
@dataclass
class FormulationConfig:
    name: str
    params: Dict[str, Any]

Barrier functions (at least 5)
def barrier_phi(g: float, kind: str, mu: float, p: float = 2.0, k: float = 10.0, eps: float = 1e-12) -> float:
    """
    Barrier term contribution for a single inequality slack g(x) > 0.
    All return +inf if g <= 0 (outside feasible region).
    """
    if g <= 0.0:
        return float("inf")

    if kind == "log":
        return -mu * math.log(max(g, eps))
    if kind == "reciprocal":
        return mu * (1.0 / max(g, eps))
    if kind == "reciprocal_square":
        return mu * (1.0 / max(g, eps)**2)
    if kind == "power":
        return mu * (1.0 / max(g, eps)**p)
    if kind == "exponential":
        # exp barrier that grows fast as g->0+ (since exp(k*(1/g)) is too extreme)
        return mu * math.exp(-k * g) / max(g, eps)
    raise ValueError(f"Unknown barrier kind: {kind}")

Reduced-space mapping: boundary feasibility by construction
def map_to_bounds(z: np.ndarray, bounds: List[Tuple[float, float]], kind: str = "sigmoid") -> np.ndarray:
    """
    Map unconstrained z in R^d to bounded x in [lo, hi] elementwise.
    This enforces boundary constraints exactly (reduced/manifold style).
    """
    z = np.asarray(z, dtype=float)
    x = np.empty_like(z)
    for i, (lo, hi) in enumerate(bounds):
        if kind == "sigmoid":
            s = 1.0 / (1.0 + np.exp(-z[i]))
            x[i] = lo + (hi - lo) * s
        elif kind == "tanh":
            s = 0.5 * (np.tanh(z[i]) + 1.0)
            x[i] = lo + (hi - lo) * s
        else:
            raise ValueError(kind)
    return x

Sequence Pair decoding (rigorous VLSI packing)

This is the “real” floorplanning decoder that guarantees non-overlap by construction.

def decode_sequence_pair(
    widths: np.ndarray,
    heights: np.ndarray,
    pi_plus: List[int],
    pi_minus: List[int],
    min_dx: float = 0.0,
    min_dy: float = 0.0,
) -> np.ndarray:
    """
    Rigorous sequence-pair decoding using constraint graphs and longest paths.
    Returns centre coordinates vector x = [cx0, cy0, cx1, cy1, ...].
    """
    n = len(widths)
    pos_minus = {r: i for i, r in enumerate(pi_minus)}

    # Build horizontal and vertical precedence sets
    # If i precedes j in both => i left of j (horizontal edge i->j)
    # If i precedes j in pi_plus and follows in pi_minus => i above j (vertical edge i->j)
    H_edges = [[] for _ in range(n)]
    V_edges = [[] for _ in range(n)]

    for a_idx in range(n):
        i = pi_plus[a_idx]
        for b_idx in range(a_idx + 1, n):
            j = pi_plus[b_idx]
            if pos_minus[i] < pos_minus[j]:
                H_edges[i].append(j)
            else:
                V_edges[i].append(j)

    # Longest path DP in topological order pi_plus
    x_left = np.zeros(n, dtype=float)
    y_bottom = np.zeros(n, dtype=float)

    # Horizontal: x_left[j] = max_i (x_left[i] + w_i + min_dx) over i->j
    for i in pi_plus:
        base = x_left[i] + widths[i] + min_dx
        for j in H_edges[i]:
            if base > x_left[j]:
                x_left[j] = base

    # Vertical: y_bottom[j] = max_i (y_bottom[i] + h_i + min_dy) over i->j
    for i in pi_plus:
        base = y_bottom[i] + heights[i] + min_dy
        for j in V_edges[i]:
            if base > y_bottom[j]:
                y_bottom[j] = base

    # Convert left/bottom to centres
    cx = x_left + widths / 2.0
    cy = y_bottom + heights / 2.0

    out = np.zeros(2 * n, dtype=float)
    out[0::2] = cx
    out[1::2] = cy
    return out


For options 6 (B*-tree) and 7 (slicing tree) you should follow the same pattern: encode → decode → compute objective.
I’m not pasting full decoders here because they’re long, but the integration hook below supports them identically. If you want, I’ll paste the full B*-tree contour packing + slicing-tree Polish decode in the next turn.

4) FormulationEngine: one hook that all optimizers call
class FormulationEngine:
    def __init__(self, problem: PlacementProblem, cfg: FormulationConfig, manager_ref):
        self.problem = problem
        self.cfg = cfg
        self.manager = manager_ref  # to call register_step / access stop/pause

    # --- decision space API (allows reduced-space & discrete encodings) ---
    def decision_dim(self) -> int:
        name = self.cfg.name
        if name.startswith("4. Reduced"):
            return self.problem.num_variables()  # z has same dim as x, but unbounded
        if name.startswith("5. Discrete encoding: Sequence Pair"):
            return 2 * self.problem.n_rects  # random keys for pi+ and pi-
        # TODO: B*-tree / slicing: may need more params; keep 2n initially
        return self.problem.num_variables()

    def decision_bounds(self) -> Optional[List[Tuple[float, float]]]:
        name = self.cfg.name
        if name.startswith("4. Reduced"):
            return None  # unconstrained (mapped to bounds internally)
        if name.startswith("5. Discrete") or name.startswith("6. Discrete") or name.startswith("7. Discrete"):
            r = float(self.cfg.params.get("key_range", 1.0))
            return [(-r, r)] * self.decision_dim()
        return self.problem.bounds()

    def decision_init(self) -> np.ndarray:
        name = self.cfg.name
        if name.startswith("4. Reduced"):
            # start at 0 => maps to centre of bounds
            return np.zeros(self.problem.num_variables(), dtype=float)
        if name.startswith("5. Discrete") or name.startswith("6. Discrete") or name.startswith("7. Discrete"):
            # random keys
            return np.random.uniform(-1.0, 1.0, size=self.decision_dim()).astype(float)
        return self.problem.initial_vector()

    # --- decode decision vector -> coordinate vector x (for metrics/plots) ---
    def decode_to_x(self, z: np.ndarray) -> np.ndarray:
        name = self.cfg.name

        if name.startswith("4. Reduced"):
            kind = self.cfg.params.get("map", "sigmoid")
            return map_to_bounds(z, self.problem.bounds(), kind=kind)

        if name.startswith("5. Discrete encoding: Sequence Pair"):
            n = self.problem.n_rects
            keys_plus = z[:n]
            keys_minus = z[n:2*n]
            pi_plus = list(np.argsort(keys_plus))
            pi_minus = list(np.argsort(keys_minus))
            cons = self.problem.constraints_cfg
            min_dx = float(cons.get("min_spacing", {}).get("min_dx", 0.0))
            min_dy = float(cons.get("min_spacing", {}).get("min_dy", 0.0))
            scale = float(self.cfg.params.get("decode_spacing", 1.0))
            widths = np.array([r.width for r in self.problem.rectangles], dtype=float)
            heights = np.array([r.height for r in self.problem.rectangles], dtype=float)
            return decode_sequence_pair(widths, heights, pi_plus, pi_minus, min_dx*scale, min_dy*scale)

        # TODO 6/7: decode B*-tree / slicing tree (same pattern)
        return np.asarray(z, dtype=float)

    # --- scalar objective used by SciPy/GA/DE/etc ---
    def evaluate_scalar(self, z: np.ndarray) -> float:
        x = self.decode_to_x(z)

        # Base objective terms
        ot = self.problem.objective_terms(x)
        f = (
            self.problem.weight_hpwl * ot["hpwl"]
            + self.problem.weight_bbox_aspect * ot["bbox_aspect"]
            + self.problem.weight_bbox_area * ot["bbox_area"]
        )

        name = self.cfg.name

        # 0) soft penalty (current)
        if name.startswith("0. Soft penalty"):
            m = self.problem.compute_metrics(x)
            return m["objective_augmented"]

        # 1) barrier constraints (interior point)
        if name.startswith("1. Barrier"):
            mu = float(self.cfg.params.get("mu", 1.0))
            kind = self.cfg.params.get("barrier_type", "log")
            p = float(self.cfg.params.get("p", 2.0))
            k = float(self.cfg.params.get("k", 10.0))
            eps = float(self.cfg.params.get("feas_eps", 1e-12))

            # slacks g_k(x) > 0
            gs = self.problem.inequality_slacks_g(x, smooth_k=10.0)

            B = 0.0
            for _, g in gs:
                Bk = barrier_phi(g - eps, kind=kind, mu=mu, p=p, k=k)
                if not math.isfinite(Bk):
                    return float("inf")
                B += Bk

            # optionally keep soft penalties for "soft constraints"
            # (future constraints can choose barrier vs penalty; keep simple here)
            return f + B

        # 2/3 use solver-based “constrained” runs; scalar only used in fallback
        if name.startswith("2. Lexicographic") or name.startswith("3. Feasible"):
            # fallback scalar if a non-constrained optimizer is used:
            V = self.problem.total_violation_scalar(x)
            # BIG dominance to enforce lexicographic priority in fallback mode:
            return 1e6 * V + f

        # 4 reduced-space already handled in decode; now just evaluate soft penalty or pure f
        if name.startswith("4. Reduced"):
            # boundary feasibility already ensured by mapping; keep penalties for others
            m = self.problem.compute_metrics(x)
            return m["objective_augmented"]

        # 5/6/7 discrete encodings -> objective in decoded x + boundary penalty if overflow
        if name.startswith("5. Discrete") or name.startswith("6. Discrete") or name.startswith("7. Discrete"):
            m = self.problem.compute_metrics(x)
            return m["objective_augmented"]

        # 8/9 solved directly via solver in a separate path (not scalar evaluation)
        if name.startswith("8. MIP") or name.startswith("9. CP"):
            return float("inf")

        return f

5) Where to call it from (minimal changes)
A) In on_run_click, create the formulation config and pass into manager

Add:

def _collect_formulation_params() -> Dict[str, Any]:
    name = formulation_select.value
    params = {}
    for w in FORM_PARAM_WIDGETS.get(name, []):
        params[w.name] = w.value
    return params


Then in on_run_click:

form_cfg = FormulationConfig(
    name=formulation_select.value,
    params=_collect_formulation_params()
)
manager.set_formulation(form_cfg)   # NEW method you add to manager

B) Add to OptimizationManager.__init__:
self.formulation_cfg: Optional[FormulationConfig] = None
self.formulation: Optional[FormulationEngine] = None

C) Add to OptimizationManager:
def set_formulation(self, cfg: FormulationConfig):
    self.formulation_cfg = cfg
    if self.problem is not None:
        self.formulation = FormulationEngine(self.problem, cfg, self)

D) Replace objective calls everywhere

Replace your existing:

def _objective_augmented_np(self, x):
    return self.problem.compute_metrics(x)["objective_augmented"]


with:

def _objective_augmented_np(self, z: np.ndarray) -> float:
    return self.formulation.evaluate_scalar(z)


And similarly _objective_augmented_np_checked(...).

E) Update start vectors and bounds (so discrete/reduced formulations work)

In start_run(...) replace:

x0 = self.problem.initial_vector() with z0 = self.formulation.decision_init()

bounds = self.problem.bounds() with bounds = self.formulation.decision_bounds()

and pass z0 everywhere the optimizers expect an initial vector.

F) Ensure plots/logs still show real coordinates

Modify register_step signature to accept decoded x:

def register_step(self, algo_name: str, z: np.ndarray, target_obj: float = 0.0, decoded_x: Optional[np.ndarray] = None):
    x = decoded_x if decoded_x is not None else z
    metrics = self.problem.compute_metrics(x)
    ...
    rec["x_vector"] = x.tolist()           # keep x_vector as coordinates for plotting
    rec["decision_vector"] = z.tolist()    # NEW: for resume/debug


Then in each optimizer callback, do:

x = self.formulation.decode_to_x(z)
self.register_step(algo_name, z, target_obj, decoded_x=x)


For continuous formulations where z==x, you can just call register_step(algo, z, ...) unchanged.

6) Solver-based formulations (2,3,8,9): don’t fake them—run the right solver
Option 2: Lexicographic (two-stage) — rigorous

Add a new worker path in _run_optimizer_worker:

if self.formulation_cfg.name.startswith("2. Lexicographic"):
    self._run_lexicographic(params, z0, target_obj)
    return


Implement:

from scipy.optimize import NonlinearConstraint

def _run_lexicographic(self, opt_params, z0, target_obj):
    eps = float(self.formulation_cfg.params.get("eps", 1e-4))
    phase1_maxiter = int(self.formulation_cfg.params.get("phase1_maxiter", 200))
    phase2_solver = self.formulation_cfg.params.get("phase2_solver", "SLSQP")

    # Phase 1: minimize V(x)
    def V_of_z(z):
        x = self.formulation.decode_to_x(z)
        if self._stop_requested(): raise StopRequested()
        return self.problem.total_violation_scalar(x)

    # Run Phase1 with selected optimizer (or force DE/DA); simplest: use scipy minimize
    res1 = minimize(V_of_z, z0, method="Powell", options={"maxiter": phase1_maxiter})
    z_feas = np.array(res1.x, dtype=float)
    Vmin = V_of_z(z_feas)

    # Phase 2: minimize f(x) subject to V(x) <= Vmin + eps
    def f_of_z(z):
        if self._stop_requested(): raise StopRequested()
        return self.formulation.evaluate_scalar(z)  # uses chosen formulation's scalar; for lex we want pure objective
    # Better: use pure objective only:
    def pure_f(z):
        x = self.formulation.decode_to_x(z)
        ot = self.problem.objective_terms(x)
        return (self.problem.weight_hpwl*ot["hpwl"]
                + self.problem.weight_bbox_aspect*ot["bbox_aspect"]
                + self.problem.weight_bbox_area*ot["bbox_area"])

    def V_constraint(z):
        return (Vmin + eps) - self.problem.total_violation_scalar(self.formulation.decode_to_x(z))

    cons = [{"type": "ineq", "fun": V_constraint}]

    method = "SLSQP" if phase2_solver == "SLSQP" else "trust-constr"
    res2 = minimize(pure_f, z_feas, method=method, constraints=cons, options={"maxiter": opt_params.get("maxiter", 200)})

    z_best = np.array(res2.x, dtype=float)
    x_best = self.formulation.decode_to_x(z_best)
    self.register_step("Lexicographic (final)", z_best, target_obj, decoded_x=x_best)


This is true lexicographic: first minimize violation, then constrain violation and optimize objective.

Option 3: Feasible direction / active-set (SQP-style) — use SLSQP

Again, don’t reduce this to a scalar penalty. Use SciPy’s SQP solver.

In _run_optimizer_worker, if formulation starts with option 3, force a constrained solver:

if self.formulation_cfg.name.startswith("3. Feasible"):
    self._run_sqp_feasible(params, z0, target_obj)
    return


Implement:

def _run_sqp_feasible(self, params, z0, target_obj):
    method = self.formulation_cfg.params.get("phase2_solver", "SLSQP")

    # Build inequality constraints g_k(z) >= 0
    def make_gk_fun(idx):
        def gk(z):
            if self._stop_requested(): raise StopRequested()
            x = self.formulation.decode_to_x(z)
            gs = self.problem.inequality_slacks_g(x, smooth_k=10.0)
            return gs[idx][1]
        return gk

    # pick a manageable subset or all constraints:
    # WARNING: all pairwise sep constraints is O(n^2) – for many rects it’s huge.
    # For chip floorplanning, consider only "critical" constraints or use discrete encodings / CP-SAT.
    gs0 = self.problem.inequality_slacks_g(self.formulation.decode_to_x(z0), smooth_k=10.0)
    constraints = [{"type": "ineq", "fun": make_gk_fun(i)} for i in range(len(gs0))]

    def pure_f(z):
        x = self.formulation.decode_to_x(z)
        ot = self.problem.objective_terms(x)
        return (self.problem.weight_hpwl*ot["hpwl"]
                + self.problem.weight_bbox_aspect*ot["bbox_aspect"]
                + self.problem.weight_bbox_area*ot["bbox_area"])

    def callback(z):
        x = self.formulation.decode_to_x(z)
        self.register_step("SLSQP (feasible)", np.array(z), target_obj, decoded_x=x)
        return self._check_pause_and_stop()

    minimize(
        pure_f, z0, method="SLSQP" if method=="SLSQP" else "trust-constr",
        constraints=constraints,
        callback=callback,
        options={"maxiter": int(params.get("maxiter", 200))}
    )


That’s a feasible-direction / active-set style approach without you implementing SQP yourself.

Option 8: MILP (MIP/MILP) — use a solver (OR-Tools MIP)

This is a solver path, not a scalar objective path.

Add:

if self.formulation_cfg.name.startswith("8. MIP"):
    self._run_milp(params, target_obj)
    return


Implementation note (important):

HPWL and bbox bounds are linearizable.

BBox area is bilinear; MILP needs approximation or MIQP/SCIP/Gurobi.
So option 8 should either:

(A) drop bbox_area for MILP, or

(B) piecewise-linear approximation, or

(C) require SCIP/Gurobi.

I recommend: MILP uses linear objective terms only, CP-SAT handles the full nonlinear area via multiplication (option 9).

Option 9: CP-SAT packing — OR-Tools NoOverlap2D + multiplication

This is the best “chip floorplanning” style for hard packing constraints.

Add:

if self.formulation_cfg.name.startswith("9. CP"):
    self._run_cpsat(params, target_obj)
    return


Key idea:

integer centers and sizes with scale factor int_scale

AddNoOverlap2D or AddNoOverlap2D([interval_x],[interval_y])

objective terms via:

x_min/x_max/y_min/y_max (linear)

hpwl = (x_max - x_min) + (y_max - y_min) (linear)

area = W*H using AddMultiplicationEquality(area, [W, H]) (exact)

aspect ratio constraint: W <= thr*H if threshold rational (linear)

Also: CP-SAT can stream solutions via a callback → live updates.

7) Compatibility reality (so you don’t paint yourself into a corner)

0 Soft penalty: works with all optimizers (current)

1 Barrier: works with continuous optimizers; best with Adam + line search / trust-constr

2 Lexicographic: implemented as two-phase solver run (rigorous); phase 2 should use SLSQP/trust-constr

3 Active-set/SQP: should use SLSQP/trust-constr (rigorous)

4 Reduced-space: works with all continuous optimizers (you’re just changing parameterization)

5/6/7 Discrete encodings: work best with GA/DE/annealing; continuous optimizers can run on “random keys” but are non-smooth

8 MILP: is its own solver path

9 CP-SAT: is its own solver path (best for exact packing)

This still keeps your UI structure: formulation dropdown chooses how the optimization problem is defined, and the backend either:

produces a scalar objective for existing optimizers, or

routes to a dedicated solver run for MILP/CP-SAT (because that’s what “full rigour” means).

8) What you must change in one place to make everything future-proof

All future constraints/objectives should be added in only two places:

PlacementProblem.objective_terms(x) / constraint_terms(x) (for plotting, logs, lexicographic V(x))

PlacementProblem.inequality_slacks_g(x) (only if you want that constraint to be “hard” under barriers / SQP / CP-SAT)

That’s the smallest change that keeps your system extensible.

If you want, I can paste the full B*-tree contour packing decoder and full slicing-tree Polish-expression decoder next (they’re longer but they fit into the same decode_to_x() hook exactly like sequence pair).














Below are drop-in, fully-implemented decoders for:

B*-tree + contour packing (option 6)

Slicing tree via NPE (Normalized Polish Expression) + deterministic packing (option 7)

…and the smallest integration edits needed inside your existing FormulationEngine.decode_to_x().

I’m also listing other discrete floorplanning encodings in the same sub-domain (besides B*-tree and slicing trees).

A) Code: B*-tree contour packing decoder (full)
1) B*-tree generation from random keys (deterministic)

This gives you a valid binary tree for any key vector, so GA/DE/SA can optimize the keys.

import math
import numpy as np
from dataclasses import dataclass
from typing import List, Tuple, Dict, Optional

@dataclass
class BStarTree:
    """Binary tree arrays describing a B*-tree."""
    root: int
    left: np.ndarray   # left[i]  = left child index or -1
    right: np.ndarray  # right[i] = right child index or -1

def _sigmoid01(x: float) -> float:
    # stable sigmoid into (0,1)
    if x >= 0:
        z = math.exp(-x)
        return 1.0 / (1.0 + z)
    else:
        z = math.exp(x)
        return z / (1.0 + z)

def random_keys_to_bstar_tree(keys_order: np.ndarray, keys_attach: np.ndarray) -> BStarTree:
    """
    Deterministically maps (keys_order, keys_attach) -> a valid B*-tree.

    - keys_order: decides insertion order of blocks
    - keys_attach: decides parent selection and whether attach as left-child or right-child

    This is designed for metaheuristics (GA/DE/SA): they mutate real keys; we decode to a tree.
    """
    n = len(keys_order)
    assert len(keys_attach) == n, "keys_attach must have length n"

    # Insertion order (root is first)
    order = list(np.argsort(keys_order))  # permutation of 0..n-1

    left = -np.ones(n, dtype=int)
    right = -np.ones(n, dtype=int)

    root = order[0]
    placed = [root]  # list of already-in-tree nodes (in insertion order)

    # Helper to find an available slot deterministically
    def attach_to_parent(child: int, parent: int, prefer_left: bool) -> bool:
        """Try attach child to parent; return True if attached else False."""
        if prefer_left:
            if left[parent] == -1:
                left[parent] = child
                return True
            if right[parent] == -1:
                right[parent] = child
                return True
        else:
            if right[parent] == -1:
                right[parent] = child
                return True
            if left[parent] == -1:
                left[parent] = child
                return True
        return False

    for k in range(1, n):
        child = order[k]
        a = float(keys_attach[child])

        # parent selection among already placed nodes
        # map attach-key -> index in [0, len(placed)-1]
        u = _sigmoid01(a)  # (0,1)
        p_idx = int(u * len(placed))
        if p_idx >= len(placed):
            p_idx = len(placed) - 1
        parent0 = placed[p_idx]

        prefer_left = (a >= 0.0)

        # If chosen parent is full, linearly probe other parents (deterministic)
        attached = False
        for t in range(len(placed)):
            parent = placed[(p_idx + t) % len(placed)]
            if attach_to_parent(child, parent, prefer_left):
                attached = True
                break

        # As a last resort (should be extremely rare), attach to last placed node
        if not attached:
            parent = placed[-1]
            if left[parent] == -1:
                left[parent] = child
            elif right[parent] == -1:
                right[parent] = child
            else:
                # If even that fails, force rewire: attach as left of root (tree remains connected)
                # (This is defensive; usually never triggers)
                if left[root] == -1:
                    left[root] = child
                else:
                    right[root] = child

        placed.append(child)

    return BStarTree(root=root, left=left, right=right)

2) Contour data structure + packing

This is the classic contour packing used for B*-trees.

# -------------------- Contour utilities (B*-tree packing) --------------------

# Contour is a sorted list of segments: (x0, x1, height), non-overlapping, adjacent may merge.
Contour = List[Tuple[float, float, float]]

def contour_init(xmax: float = 1e9) -> Contour:
    # Start with flat skyline at y=0 from x=0..xmax
    return [(0.0, float(xmax), 0.0)]

def contour_query_max(contour: Contour, x0: float, x1: float) -> float:
    """Return max height over interval [x0, x1] on the contour."""
    if x1 <= x0:
        return 0.0
    m = 0.0
    for a, b, h in contour:
        if b <= x0:
            continue
        if a >= x1:
            break
        # overlap exists
        if h > m:
            m = h
    return m

def contour_update(contour: Contour, x0: float, x1: float, new_h: float) -> Contour:
    """
    Set contour height over [x0, x1] to new_h, leaving rest unchanged.
    Returns a new merged contour.
    """
    if x1 <= x0:
        return contour

    out: Contour = []
    for a, b, h in contour:
        if b <= x0 or a >= x1:
            # no overlap with [x0,x1]
            out.append((a, b, h))
            continue

        # left remainder
        if a < x0:
            out.append((a, x0, h))

        # middle replaced
        mid_a = max(a, x0)
        mid_b = min(b, x1)
        out.append((mid_a, mid_b, new_h))

        # right remainder
        if b > x1:
            out.append((x1, b, h))

    # sort and merge adjacent equal heights
    out.sort(key=lambda t: t[0])
    merged: Contour = []
    for seg in out:
        if not merged:
            merged.append(seg)
            continue
        a, b, h = seg
        pa, pb, ph = merged[-1]
        if abs(ph - h) < 1e-12 and abs(pb - a) < 1e-12:
            merged[-1] = (pa, b, ph)
        else:
            merged.append(seg)

    # Also merge any accidental overlaps by trimming (defensive)
    cleaned: Contour = []
    last_end = None
    for a, b, h in merged:
        if last_end is None:
            cleaned.append((a, b, h))
            last_end = b
            continue
        if a < last_end:
            a = last_end
        if b > a:
            cleaned.append((a, b, h))
            last_end = b
    return cleaned

3) Full B*-tree decode to coordinates (with min spacing via inflation + canvas-centering)
def decode_bstar_tree_contour(
    widths: np.ndarray,
    heights: np.ndarray,
    tree: BStarTree,
    min_dx: float = 0.0,
    min_dy: float = 0.0,
    canvas_w: Optional[float] = None,
    canvas_h: Optional[float] = None,
) -> np.ndarray:
    """
    Decode B*-tree into non-overlapping placement using contour packing.

    IMPORTANT spacing handling:
    - We "inflate" each block to (w+min_dx, h+min_dy) for packing.
      If inflated blocks do not overlap, original blocks satisfy min spacing in x and y.

    Returns:
      x vector of centres: [cx0, cy0, cx1, cy1, ...]
    """
    n = len(widths)
    assert len(heights) == n
    left = tree.left
    right = tree.right
    root = int(tree.root)

    # Inflated dims for spacing-by-construction
    infl_w = widths + float(min_dx)
    infl_h = heights + float(min_dy)

    # Arrays store inflated bottom-left positions
    x_bl = np.zeros(n, dtype=float)
    y_bl = np.zeros(n, dtype=float)

    # parent-driven x rules (B*-tree semantics):
    # left-child => place to the RIGHT of parent
    # right-child => align x with parent (then y comes from contour)
    parent = -np.ones(n, dtype=int)

    # Build parent pointers with DFS
    stack = [root]
    while stack:
        u = stack.pop()
        lc = int(left[u])
        rc = int(right[u])
        if lc != -1:
            parent[lc] = u
            stack.append(lc)
        if rc != -1:
            parent[rc] = u
            stack.append(rc)

    # Traverse in a stable BFS/DFS order from root to ensure parents placed before children
    order: List[int] = []
    q = [root]
    while q:
        u = q.pop(0)
        order.append(u)
        lc = int(left[u]); rc = int(right[u])
        if lc != -1: q.append(lc)
        if rc != -1: q.append(rc)

    # Contour over inflated blocks
    contour = contour_init(xmax=1e9)

    # Place root at x=0; y from contour => 0
    x_bl[root] = 0.0
    y_bl[root] = contour_query_max(contour, x_bl[root], x_bl[root] + infl_w[root])
    contour = contour_update(contour, x_bl[root], x_bl[root] + infl_w[root], y_bl[root] + infl_h[root])

    # Place remaining blocks
    for u in order[1:]:
        p = int(parent[u])
        if p < 0:
            # disconnected defensive fallback: place at far right
            x0 = float(np.max(x_bl + infl_w) + 1.0)
        else:
            # Determine if u is left-child or right-child of p
            if int(left[p]) == u:
                x0 = x_bl[p] + infl_w[p]   # to the right of parent
            else:
                x0 = x_bl[p]               # aligned with parent

        x_bl[u] = x0
        y_bl[u] = contour_query_max(contour, x0, x0 + infl_w[u])
        contour = contour_update(contour, x0, x0 + infl_w[u], y_bl[u] + infl_h[u])

    # Convert inflated bottom-left to ORIGINAL centres by removing half-inflation margin
    cx = x_bl + (min_dx / 2.0) + (widths / 2.0)
    cy = y_bl + (min_dy / 2.0) + (heights / 2.0)

    # Optional: translate to center inside canvas if it fits
    if canvas_w is not None and canvas_h is not None:
        xmin = float(np.min(cx - widths/2.0))
        xmax = float(np.max(cx + widths/2.0))
        ymin = float(np.min(cy - heights/2.0))
        ymax = float(np.max(cy + heights/2.0))
        bw = xmax - xmin
        bh = ymax - ymin

        if bw <= canvas_w:
            dx = (canvas_w - bw) * 0.5 - xmin
        else:
            dx = -xmin  # just shift to start at 0; still violates boundary if too large

        if bh <= canvas_h:
            dy = (canvas_h - bh) * 0.5 - ymin
        else:
            dy = -ymin

        cx = cx + dx
        cy = cy + dy

    out = np.zeros(2*n, dtype=float)
    out[0::2] = cx
    out[1::2] = cy
    return out

B) Code: Full slicing-tree decoder (NPE) + packing

This uses Normalized Polish Expression (NPE): a list of length 2n-1 containing:

n operands (module ids)

n-1 operators in { 'H', 'V' }

The decoding is deterministic and guarantees a slicing structure.

1) Generate a valid NPE from keys (deterministic)
def random_keys_to_npe(keys_ops: np.ndarray, keys_ctrl: np.ndarray) -> List:
    """
    Deterministically builds a valid NPE for n modules.

    keys_ops: defines module permutation (operands order)
    keys_ctrl: controls when to emit operators and operator types

    Returns an NPE list like: [3, 1, 'V', 0, 'H', 2, 'V', ...]
    """
    n = len(keys_ops)
    assert len(keys_ctrl) == n, "keys_ctrl must have length n"

    operands = list(np.argsort(keys_ops))  # permutation of [0..n-1]

    # operator types for the n-1 operators
    # use the first (n-1) ctrl keys for operator types
    op_types = [('H' if keys_ctrl[i] >= 0.0 else 'V') for i in range(n-1)]

    # We must satisfy the NPE ballot property:
    # in any prefix: #operands >= #operators + 1
    # and at end: operands=n, operators=n-1.
    expr: List = []
    n_opnd = 0
    n_oper = 0

    # Use remaining keys to decide "emit operator now?" as a threshold schedule
    # Larger key => emit operators earlier (more compact tree)
    # We'll map ctrl keys into [0,1] and compare against stack depth.
    ctrl01 = [_sigmoid01(float(keys_ctrl[i])) for i in range(n)]
    t_idx = 0

    for i, m in enumerate(operands):
        expr.append(int(m))
        n_opnd += 1

        # after adding an operand, we may be able to add operators
        # allowed if we have at least 2 items on the implicit stack: n_opnd - n_oper >= 2
        while (n_oper < n-1) and ((n_opnd - n_oper) >= 2):
            # decide whether to emit an operator now
            # heuristic: if ctrl indicates "compact", emit; else delay
            depth = n_opnd - n_oper
            # threshold increases with depth; deeper => more willing to emit
            prob_emit = min(1.0, 0.25 + 0.15 * (depth - 2))
            key = ctrl01[(t_idx + i) % n]
            t_idx += 1
            if key < prob_emit:
                op = op_types[n_oper]
                expr.append(op)
                n_oper += 1
            else:
                break

    # Emit remaining operators
    while n_oper < n-1:
        expr.append(op_types[n_oper])
        n_oper += 1

    # Defensive validity check; if somehow invalid, fix by a standard repair:
    if not _is_valid_npe(expr, n):
        expr = _repair_npe(expr, n)

    return expr

def _is_valid_npe(expr: List, n: int) -> bool:
    n_opnd = 0
    n_oper = 0
    for tok in expr:
        if isinstance(tok, int):
            n_opnd += 1
        else:
            n_oper += 1
        if n_opnd < n_oper + 1:
            return False
    return (n_opnd == n) and (n_oper == n-1) and (len(expr) == 2*n - 1)

def _repair_npe(expr: List, n: int) -> List:
    """
    Simple repair: rebuild from operands order in expr and reuse operator types,
    then force a canonical valid NPE: operands in order, then operators interleaved.
    """
    ops = [t for t in expr if isinstance(t, int)]
    if len(ops) != n:
        ops = list(range(n))

    oper_types = [t for t in expr if not isinstance(t, int)]
    if len(oper_types) != n-1:
        oper_types = ['V']*(n-1)

    # Canonical valid NPE: push first two operands, operator, then next operand, operator, ...
    out: List = [ops[0], ops[1], oper_types[0]]
    k = 2
    for j in range(1, n-1):
        out.append(ops[k]); k += 1
        out.append(oper_types[j])
    return out

2) Decode NPE to module coordinates (full)
@dataclass
class _SubFloorplan:
    """A composite floorplan piece produced during NPE decoding."""
    w: float
    h: float
    # module -> (x_bl, y_bl) bottom-left in composite coordinates
    pos: Dict[int, Tuple[float, float]]

def decode_slicing_npe(
    widths: np.ndarray,
    heights: np.ndarray,
    npe: List,
    min_dx: float = 0.0,
    min_dy: float = 0.0,
    canvas_w: Optional[float] = None,
    canvas_h: Optional[float] = None,
) -> np.ndarray:
    """
    Decode a slicing floorplan from an NPE.

    Spacing:
    - Inflate each module to (w+min_dx, h+min_dy) during packing.
    - Then convert inflated bottom-left to original centre coords.

    Operators:
    - 'H': stack pop B, pop A => place B above A
    - 'V': stack pop B, pop A => place B right of A
    """
    n = len(widths)
    infl_w = widths + float(min_dx)
    infl_h = heights + float(min_dy)

    stack: List[_SubFloorplan] = []

    for tok in npe:
        if isinstance(tok, int):
            i = int(tok)
            fp = _SubFloorplan(
                w=float(infl_w[i]),
                h=float(infl_h[i]),
                pos={i: (0.0, 0.0)}
            )
            stack.append(fp)
        else:
            op = tok
            if len(stack) < 2:
                # invalid expression; bail to a trivial packing
                return _fallback_line_pack(widths, heights, min_dx, min_dy, canvas_w, canvas_h)

            B = stack.pop()
            A = stack.pop()

            if op == 'H':
                # B above A
                w = max(A.w, B.w)
                h = A.h + B.h

                pos: Dict[int, Tuple[float, float]] = {}
                # A stays at (0,0)
                for k, (x, y) in A.pos.items():
                    pos[k] = (x, y)
                # B shifted by (0, A.h)
                for k, (x, y) in B.pos.items():
                    pos[k] = (x, y + A.h)

                stack.append(_SubFloorplan(w=w, h=h, pos=pos))

            elif op == 'V':
                # B right of A
                w = A.w + B.w
                h = max(A.h, B.h)

                pos = {}
                for k, (x, y) in A.pos.items():
                    pos[k] = (x, y)
                for k, (x, y) in B.pos.items():
                    pos[k] = (x + A.w, y)

                stack.append(_SubFloorplan(w=w, h=h, pos=pos))
            else:
                # unknown operator
                return _fallback_line_pack(widths, heights, min_dx, min_dy, canvas_w, canvas_h)

    if len(stack) != 1:
        return _fallback_line_pack(widths, heights, min_dx, min_dy, canvas_w, canvas_h)

    fp = stack[0]
    # fp.pos contains inflated bottom-lefts
    cx = np.zeros(n, dtype=float)
    cy = np.zeros(n, dtype=float)

    for i in range(n):
        x_bl, y_bl = fp.pos.get(i, (0.0, 0.0))
        # inflated bottom-left -> original centre
        cx[i] = x_bl + (min_dx / 2.0) + (widths[i] / 2.0)
        cy[i] = y_bl + (min_dy / 2.0) + (heights[i] / 2.0)

    # Optional translate to center inside canvas if possible
    if canvas_w is not None and canvas_h is not None:
        xmin = float(np.min(cx - widths/2.0))
        xmax = float(np.max(cx + widths/2.0))
        ymin = float(np.min(cy - heights/2.0))
        ymax = float(np.max(cy + heights/2.0))
        bw = xmax - xmin
        bh = ymax - ymin

        dx = ((canvas_w - bw) * 0.5 - xmin) if (bw <= canvas_w) else (-xmin)
        dy = ((canvas_h - bh) * 0.5 - ymin) if (bh <= canvas_h) else (-ymin)

        cx += dx
        cy += dy

    out = np.zeros(2*n, dtype=float)
    out[0::2] = cx
    out[1::2] = cy
    return out

def _fallback_line_pack(
    widths: np.ndarray,
    heights: np.ndarray,
    min_dx: float,
    min_dy: float,
    canvas_w: Optional[float],
    canvas_h: Optional[float],
) -> np.ndarray:
    """
    Defensive fallback: place modules in a single row left-to-right.
    Always returns something.
    """
    n = len(widths)
    x = 0.0
    cx = np.zeros(n, dtype=float)
    cy = np.zeros(n, dtype=float)
    y0 = 0.0

    for i in range(n):
        cx[i] = x + (min_dx/2.0) + widths[i]/2.0
        cy[i] = y0 + (min_dy/2.0) + heights[i]/2.0
        x += widths[i] + min_dx

    if canvas_w is not None and canvas_h is not None:
        # shift into canvas if possible
        xmin = float(np.min(cx - widths/2.0))
        ymin = float(np.min(cy - heights/2.0))
        cx += -xmin
        cy += -ymin

    out = np.zeros(2*n, dtype=float)
    out[0::2] = cx
    out[1::2] = cy
    return out

C) Minimal integration: update FormulationEngine.decode_to_x()

Inside your existing FormulationEngine.decode_to_x(self, z) add/replace the TODO parts:

# inside FormulationEngine.decode_to_x(self, z):

if name.startswith("6. Discrete encoding: B*-tree"):
    n = self.problem.n_rects
    keys_order = z[:n]
    keys_attach = z[n:2*n]
    tree = random_keys_to_bstar_tree(keys_order, keys_attach)

    cons = self.problem.constraints_cfg
    min_dx = float(cons.get("min_spacing", {}).get("min_dx", 0.0))
    min_dy = float(cons.get("min_spacing", {}).get("min_dy", 0.0))

    widths = np.array([r.width for r in self.problem.rectangles], dtype=float)
    heights = np.array([r.height for r in self.problem.rectangles], dtype=float)

    scale = float(self.cfg.params.get("decode_spacing", 1.0))
    return decode_bstar_tree_contour(
        widths, heights, tree,
        min_dx=min_dx*scale,
        min_dy=min_dy*scale,
        canvas_w=self.problem.canvas_width,
        canvas_h=self.problem.canvas_height,
    )

if name.startswith("7. Discrete encoding: Slicing tree"):
    n = self.problem.n_rects
    keys_ops = z[:n]
    keys_ctrl = z[n:2*n]
    npe = random_keys_to_npe(keys_ops, keys_ctrl)

    cons = self.problem.constraints_cfg
    min_dx = float(cons.get("min_spacing", {}).get("min_dx", 0.0))
    min_dy = float(cons.get("min_spacing", {}).get("min_dy", 0.0))

    widths = np.array([r.width for r in self.problem.rectangles], dtype=float)
    heights = np.array([r.height for r in self.problem.rectangles], dtype=float)

    scale = float(self.cfg.params.get("decode_spacing", 1.0))
    return decode_slicing_npe(
        widths, heights, npe,
        min_dx=min_dx*scale,
        min_dy=min_dy*scale,
        canvas_w=self.problem.canvas_width,
        canvas_h=self.problem.canvas_height,
    )


That’s it—your existing optimizers will now be able to run on these discrete representations because they only need a scalar objective, and you already do x = decode_to_x(z) before metrics/plots.












For chip floorplanning, the three from my list that are most practical / widely used (and complement what you already have: Sequence Pair, B*-tree, Slicing) are:

O-tree – compact search space + linear-time packing with a contour; strong in classic VLSI floorplanning literature for “admissible placements” and fast decoding.

Corner Block List / Corner Sequence (CBL/CS) – also compact with linear-time (or near-linear) decoding; used a lot in VLSI packing/floorplanning.

TCG / TCG-S family – best when you want general non-slicing floorplans and explicit horizontal/vertical precedence modeling; it’s a “graph-first” view and is a standard representation in VLSI floorplanning papers.

Below is a drop-in implementation for these three, designed to match your existing pattern:

random_keys -> encoding

decode(encoding, widths, heights, min_dx, min_dy, canvas_w, canvas_h) -> x_vector centres

You can paste this directly into your main file (near your existing Sequence Pair / B*-tree / slicing decoders), then add 3 small decode_to_x() cases at the end.

1) Drop-in code: shared helpers (sigmoid, contour, centering)
import math
import numpy as np
from dataclasses import dataclass
from typing import List, Tuple, Dict, Optional

# -------------------- Shared helpers --------------------

def _sigmoid01(x: float) -> float:
    """Stable sigmoid mapping R -> (0,1)."""
    if x >= 0:
        z = math.exp(-x)
        return 1.0 / (1.0 + z)
    z = math.exp(x)
    return z / (1.0 + z)

# Contour is a sorted list of segments: (x0, x1, height)
Contour = List[Tuple[float, float, float]]

def contour_init(xmax: float = 1e9) -> Contour:
    """Start with flat skyline y=0 for x in [0, xmax)."""
    return [(0.0, float(xmax), 0.0)]

def contour_query_max(contour: Contour, x0: float, x1: float) -> float:
    """Max skyline height over [x0, x1]."""
    if x1 <= x0:
        return 0.0
    m = 0.0
    for a, b, h in contour:
        if b <= x0:
            continue
        if a >= x1:
            break
        if h > m:
            m = h
    return m

def contour_update(contour: Contour, x0: float, x1: float, new_h: float) -> Contour:
    """Set skyline height over [x0, x1] to new_h and merge adjacent segments."""
    if x1 <= x0:
        return contour

    out: Contour = []
    for a, b, h in contour:
        if b <= x0 or a >= x1:
            out.append((a, b, h))
            continue

        if a < x0:
            out.append((a, x0, h))

        mid_a = max(a, x0)
        mid_b = min(b, x1)
        out.append((mid_a, mid_b, new_h))

        if b > x1:
            out.append((x1, b, h))

    out.sort(key=lambda t: t[0])

    merged: Contour = []
    for a, b, h in out:
        if not merged:
            merged.append((a, b, h))
            continue
        pa, pb, ph = merged[-1]
        if abs(ph - h) < 1e-12 and abs(pb - a) < 1e-12:
            merged[-1] = (pa, b, ph)
        else:
            merged.append((a, b, h))
    return merged

def contour_breakpoints(contour: Contour) -> List[float]:
    """Candidate 'corner x' positions: start of each contour segment."""
    xs = []
    for a, _, _ in contour:
        xs.append(float(a))
    # unique + sorted
    xs = sorted(set(xs))
    return xs

def _center_into_canvas(cx: np.ndarray, cy: np.ndarray,
                        widths: np.ndarray, heights: np.ndarray,
                        canvas_w: Optional[float], canvas_h: Optional[float]) -> Tuple[np.ndarray, np.ndarray]:
    """
    Translate placement to roughly center it in the canvas if it fits.
    (Does NOT scale; if it doesn't fit, it just shifts min corner to 0.)
    """
    if canvas_w is None or canvas_h is None:
        return cx, cy

    xmin = float(np.min(cx - widths / 2.0))
    xmax = float(np.max(cx + widths / 2.0))
    ymin = float(np.min(cy - heights / 2.0))
    ymax = float(np.max(cy + heights / 2.0))

    bw = xmax - xmin
    bh = ymax - ymin

    dx = ((canvas_w - bw) * 0.5 - xmin) if (bw <= canvas_w) else (-xmin)
    dy = ((canvas_h - bh) * 0.5 - ymin) if (bh <= canvas_h) else (-ymin)

    return cx + dx, cy + dy

2) O-tree: keys → O-tree → contour packing decode

Idea: choose an insertion order; each new block attaches to a parent and is placed to the right of that parent; y is taken from the skyline (contour). This yields non-overlap by construction (for inflated widths/heights).

# -------------------- O-tree --------------------

@dataclass
class OTree:
    root: int
    parent: np.ndarray  # parent[i] = parent index or -1

def random_keys_to_otree(keys_order: np.ndarray, keys_parent: np.ndarray) -> OTree:
    """
    Decode two key vectors into an O-tree:
      - keys_order decides insertion order (root is first)
      - keys_parent decides parent selection among already placed nodes
    """
    n = len(keys_order)
    assert len(keys_parent) == n

    order = list(np.argsort(keys_order))
    root = int(order[0])

    parent = -np.ones(n, dtype=int)

    placed = [root]
    for k in range(1, n):
        u = int(order[k])
        t = _sigmoid01(float(keys_parent[u]))
        p_idx = int(t * len(placed))
        if p_idx >= len(placed):
            p_idx = len(placed) - 1
        p = int(placed[p_idx])
        parent[u] = p
        placed.append(u)

    return OTree(root=root, parent=parent)

def decode_otree_contour(widths: np.ndarray, heights: np.ndarray, tree: OTree,
                         min_dx: float = 0.0, min_dy: float = 0.0,
                         canvas_w: Optional[float] = None, canvas_h: Optional[float] = None) -> np.ndarray:
    """
    O-tree packing via contour:
      - Inflate modules to enforce min spacing by construction.
      - Place root at x=0
      - For node i: x_bl = (x_bl[parent] + infl_w[parent]) , y_bl = skyline_max over its span
    """
    n = len(widths)
    infl_w = widths + float(min_dx)
    infl_h = heights + float(min_dy)

    # Build children lists to traverse parent->child
    children: List[List[int]] = [[] for _ in range(n)]
    for i in range(n):
        p = int(tree.parent[i])
        if p != -1:
            children[p].append(i)

    # BFS ensures parents placed before children
    order: List[int] = []
    q = [int(tree.root)]
    while q:
        u = q.pop(0)
        order.append(u)
        for v in children[u]:
            q.append(v)

    x_bl = np.zeros(n, dtype=float)
    y_bl = np.zeros(n, dtype=float)

    contour = contour_init()

    # Root
    r = int(tree.root)
    x_bl[r] = 0.0
    y_bl[r] = contour_query_max(contour, x_bl[r], x_bl[r] + infl_w[r])
    contour = contour_update(contour, x_bl[r], x_bl[r] + infl_w[r], y_bl[r] + infl_h[r])

    # Others
    for u in order[1:]:
        p = int(tree.parent[u])
        if p == -1:
            # defensive: place at end
            x0 = float(np.max(x_bl + infl_w) + 1.0)
        else:
            x0 = x_bl[p] + infl_w[p]

        x_bl[u] = x0
        y_bl[u] = contour_query_max(contour, x0, x0 + infl_w[u])
        contour = contour_update(contour, x0, x0 + infl_w[u], y_bl[u] + infl_h[u])

    # inflated bottom-left -> original centres
    cx = x_bl + (min_dx / 2.0) + (widths / 2.0)
    cy = y_bl + (min_dy / 2.0) + (heights / 2.0)

    cx, cy = _center_into_canvas(cx, cy, widths, heights, canvas_w, canvas_h)

    out = np.zeros(2 * n, dtype=float)
    out[0::2] = cx
    out[1::2] = cy
    return out

3) Corner Block List / Corner Sequence: skyline-corner placement

Idea: maintain a skyline contour. For each module (in order), choose one of the skyline “corners” (breakpoints) based on a key, place the block there (y from skyline), then update skyline. This is a practical “corner-sequence” style packing (CBL/CS flavor).

# -------------------- Corner sequence (CBL/CS-style) --------------------

def random_keys_to_corner_sequence(keys_order: np.ndarray, keys_corner: np.ndarray) -> Tuple[List[int], np.ndarray]:
    """
    Encode:
      - placement order from keys_order
      - corner-choice keys_corner (per-module) used during decoding
    """
    n = len(keys_order)
    assert len(keys_corner) == n
    order = list(np.argsort(keys_order))
    return order, np.asarray(keys_corner, dtype=float)

def decode_corner_sequence_contour(widths: np.ndarray, heights: np.ndarray,
                                  order: List[int], corner_keys: np.ndarray,
                                  min_dx: float = 0.0, min_dy: float = 0.0,
                                  canvas_w: Optional[float] = None, canvas_h: Optional[float] = None) -> np.ndarray:
    """
    Skyline-corner placement:
      - Inflate dims for spacing-by-construction.
      - At each step, candidate x are contour breakpoints.
      - Choose breakpoint index deterministically from corner_keys[module].
    """
    n = len(widths)
    infl_w = widths + float(min_dx)
    infl_h = heights + float(min_dy)

    x_bl = np.zeros(n, dtype=float)
    y_bl = np.zeros(n, dtype=float)

    contour = contour_init()

    for u in order:
        u = int(u)
        xs = contour_breakpoints(contour)
        if not xs:
            xs = [0.0]

        t = _sigmoid01(float(corner_keys[u]))  # (0,1)
        idx = int(t * len(xs))
        if idx >= len(xs):
            idx = len(xs) - 1
        x0 = float(xs[idx])

        x_bl[u] = x0
        y_bl[u] = contour_query_max(contour, x0, x0 + infl_w[u])
        contour = contour_update(contour, x0, x0 + infl_w[u], y_bl[u] + infl_h[u])

    # Convert to original centres
    cx = x_bl + (min_dx / 2.0) + (widths / 2.0)
    cy = y_bl + (min_dy / 2.0) + (heights / 2.0)

    cx, cy = _center_into_canvas(cx, cy, widths, heights, canvas_w, canvas_h)

    out = np.zeros(2 * n, dtype=float)
    out[0::2] = cx
    out[1::2] = cy
    return out

4) TCG-style: keys → two DAGs (Ch,Cv) → longest-path decode

This is a graph-based formulation: horizontal and vertical precedence DAGs; coordinates come from longest paths (classic in these representations).

Note: Full “TCG-S” has additional feasibility conditions and operations; this drop-in gives you a rigorous TCG-like decode that is solver-friendly and pairs well with metaheuristics (DE/GA/SA).

# -------------------- TCG-style (Ch,Cv) decode --------------------

@dataclass
class TCG:
    pi_a: List[int]          # permutation A
    pi_b: List[int]          # permutation B
    H_edges: List[List[int]] # horizontal edges i -> j (i left of j)
    V_edges: List[List[int]] # vertical edges i -> j (i below j)

def random_keys_to_tcg(keys_a: np.ndarray, keys_b: np.ndarray) -> TCG:
    """
    Build a TCG-like pair of precedence graphs from two permutations.
    We use a classic SP-derived rule to assign each pair to exactly one of (H or V),
    but we store it explicitly as two DAGs.
    """
    n = len(keys_a)
    assert len(keys_b) == n

    pi_a = list(np.argsort(keys_a))
    pi_b = list(np.argsort(keys_b))
    pos_b = {v: i for i, v in enumerate(pi_b)}

    H_edges = [[] for _ in range(n)]
    V_edges = [[] for _ in range(n)]

    # O(n^2) pair assignment: for each i precedes j in pi_a,
    # decide H vs V by relative order in pi_b.
    for ia in range(n):
        i = pi_a[ia]
        for jb in range(ia + 1, n):
            j = pi_a[jb]
            if pos_b[i] < pos_b[j]:
                # i left of j
                H_edges[i].append(j)
            else:
                # i below j  (equivalently, j above i)
                V_edges[i].append(j)

    return TCG(pi_a=pi_a, pi_b=pi_b, H_edges=H_edges, V_edges=V_edges)

def decode_tcg_longest_paths(widths: np.ndarray, heights: np.ndarray, tcg: TCG,
                            min_dx: float = 0.0, min_dy: float = 0.0,
                            canvas_w: Optional[float] = None, canvas_h: Optional[float] = None) -> np.ndarray:
    """
    Longest-path decode on DAGs:
      x_left[j] = max_{i->j in Ch} (x_left[i] + infl_w[i])
      y_bottom[j] = max_{i->j in Cv} (y_bottom[i] + infl_h[i])
    Topological order: we can safely use pi_a as a topo order for both graphs
    because edges are created only from earlier->later in pi_a.
    """
    n = len(widths)
    infl_w = widths + float(min_dx)
    infl_h = heights + float(min_dy)

    x_left = np.zeros(n, dtype=float)
    y_bottom = np.zeros(n, dtype=float)

    topo = tcg.pi_a

    # Horizontal
    for i in topo:
        base = x_left[i] + infl_w[i]
        for j in tcg.H_edges[i]:
            if base > x_left[j]:
                x_left[j] = base

    # Vertical
    for i in topo:
        base = y_bottom[i] + infl_h[i]
        for j in tcg.V_edges[i]:
            if base > y_bottom[j]:
                y_bottom[j] = base

    # Convert to original centres
    cx = x_left + (min_dx / 2.0) + (widths / 2.0)
    cy = y_bottom + (min_dy / 2.0) + (heights / 2.0)

    cx, cy = _center_into_canvas(cx, cy, widths, heights, canvas_w, canvas_h)

    out = np.zeros(2 * n, dtype=float)
    out[0::2] = cx
    out[1::2] = cy
    return out

5) Minimal integration into your current app (3 new decode cases)
Add three new UI options

Add these to your FORMULATION_OPTIONS (or wherever you list discrete encodings):

"10. Discrete encoding: O-tree",
"11. Discrete encoding: Corner Sequence (CBL/CS)",
"12. Discrete encoding: TCG (Ch/Cv)",


(If you must keep numbering tight, rename them to 6/7/8 etc. — your choice.)

Decision dimension

All three use 2n keys like your other discrete encodings:

O-tree: keys_order (n) + keys_parent (n)

Corner sequence: keys_order (n) + keys_corner (n)

TCG: keys_a (n) + keys_b (n) (two permutations)

Add these cases into FormulationEngine.decode_to_x(self, z)

Drop this in next to your existing discrete decode cases:

if name.startswith("10. Discrete encoding: O-tree"):
    n = self.problem.n_rects
    keys_order = z[:n]
    keys_parent = z[n:2*n]
    tree = random_keys_to_otree(keys_order, keys_parent)

    cons = self.problem.constraints_cfg
    min_dx = float(cons.get("min_spacing", {}).get("min_dx", 0.0))
    min_dy = float(cons.get("min_spacing", {}).get("min_dy", 0.0))

    widths = np.array([r.width for r in self.problem.rectangles], dtype=float)
    heights = np.array([r.height for r in self.problem.rectangles], dtype=float)
    scale = float(self.cfg.params.get("decode_spacing", 1.0))

    return decode_otree_contour(
        widths, heights, tree,
        min_dx=min_dx*scale, min_dy=min_dy*scale,
        canvas_w=self.problem.canvas_width, canvas_h=self.problem.canvas_height
    )

if name.startswith("11. Discrete encoding: Corner Sequence"):
    n = self.problem.n_rects
    keys_order = z[:n]
    keys_corner = z[n:2*n]
    order, corner_keys = random_keys_to_corner_sequence(keys_order, keys_corner)

    cons = self.problem.constraints_cfg
    min_dx = float(cons.get("min_spacing", {}).get("min_dx", 0.0))
    min_dy = float(cons.get("min_spacing", {}).get("min_dy", 0.0))

    widths = np.array([r.width for r in self.problem.rectangles], dtype=float)
    heights = np.array([r.height for r in self.problem.rectangles], dtype=float)
    scale = float(self.cfg.params.get("decode_spacing", 1.0))

    return decode_corner_sequence_contour(
        widths, heights, order, corner_keys,
        min_dx=min_dx*scale, min_dy=min_dy*scale,
        canvas_w=self.problem.canvas_width, canvas_h=self.problem.canvas_height
    )

if name.startswith("12. Discrete encoding: TCG"):
    n = self.problem.n_rects
    keys_a = z[:n]
    keys_b = z[n:2*n]
    tcg = random_keys_to_tcg(keys_a, keys_b)

    cons = self.problem.constraints_cfg
    min_dx = float(cons.get("min_spacing", {}).get("min_dx", 0.0))
    min_dy = float(cons.get("min_spacing", {}).get("min_dy", 0.0))

    widths = np.array([r.width for r in self.problem.rectangles], dtype=float)
    heights = np.array([r.height for r in self.problem.rectangles], dtype=float)
    scale = float(self.cfg.params.get("decode_spacing", 1.0))

    return decode_tcg_longest_paths(
        widths, heights, tcg,
        min_dx=min_dx*scale, min_dy=min_dy*scale,
        canvas_w=self.problem.canvas_width, canvas_h=self.problem.canvas_height
    )

Notes you’ll care about (practical floorplanning)

All three decoders are “packing-by-construction” using inflated dims → they enforce non-overlap + min spacing structurally (for the inflated blocks). Boundary fit is still something you can penalize or handle via canvas centering/clamping.

O-tree / Corner sequence are excellent when you need very fast decoding and a smaller search space (classic SA/GA loops).

TCG-style is great when you want a clear graph model (and later can add constraints as graph edges / precedence).
