import os
import json
import numpy as np
import pandas as pd
import plotly.graph_objects as go
import torch
from torch import nn, optim
from torch.utils.data import TensorDataset, DataLoader
from torch.nn import functional as F
from sklearn.preprocessing import StandardScaler
from sklearn.mixture import BayesianGaussianMixture
from tqdm import tqdm
import warnings
from scipy.stats import multivariate_normal

#####################################
# DEVICE CONFIGURATION
#####################################
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", device)

#####################################
# DATA LOADING & PREPROCESSING
#####################################
def load_and_preprocess_data(test_path, radar_folder, gnd_folder):
    """
    Loads radar.ftr and gnd.ftr for a given test case.
    Assumes:
      - Radar file is named as: testcase_name.ftr (in radar_folder)
      - Ground truth file is named as: testcase_name_gnd.ftr (in gnd_folder)
    Returns:
      - dataset: a TensorDataset of normalized features (for clustering)
      - clustering_feature_cols: list of feature columns used for clustering (only quality and angle features)
      - dfRadar: original radar DataFrame
      - dfGND: original ground truth DataFrame
    For clustering, kinematic quantities (e.g., Range, VrelRad, variances) are removed to reduce spatial bias.
    """
    test_name = os.path.splitext(os.path.basename(test_path))[0]
    radar_file = os.path.join(radar_folder, test_name + ".ftr")
    gnd_file = os.path.join(gnd_folder, test_name + "_gnd.ftr")
    
    dfRadar = pd.read_feather(radar_file)
    dfGND = pd.read_feather(gnd_file)
    
    # For clustering, select only quality signals and angles.
    # We remove explicit kinematic quantities that may cause spatial bias.
    clustering_feature_cols = [
        "SNR", "RCS", "aa", "bb", "cc", "dd", "ee", "ff", "gg",
        "hh", "jj", "kk", "Azimuth", "Elevation Angle"
    ]
    data = dfRadar[clustering_feature_cols].values.astype(np.float32)
    scaler = StandardScaler()
    data_scaled = scaler.fit_transform(data)
    tensor_data = torch.tensor(data_scaled, dtype=torch.float32)
    dataset = TensorDataset(tensor_data)
    
    return dataset, clustering_feature_cols, dfRadar, dfGND

#####################################
# VIB+VAE MODEL DEFINITION (Shared Encoder)
#####################################
class VIBVAE(nn.Module):
    def __init__(self, input_dim, latent_dim, hidden_dim=128):
        super(VIBVAE, self).__init__()
        # Encoder: two hidden layers then output latent mean and log-variance.
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.fc_mean = nn.Linear(hidden_dim, latent_dim)
        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)
        # Decoder: two hidden layers then reconstruct input.
        self.fc_dec1 = nn.Linear(latent_dim, hidden_dim)
        self.fc_dec2 = nn.Linear(hidden_dim, hidden_dim)
        self.fc_out = nn.Linear(hidden_dim, input_dim)
    
    def encode(self, x):
        h = F.relu(self.fc1(x))
        h = F.relu(self.fc2(h))
        mu = self.fc_mean(h)
        logvar = self.fc_logvar(h)
        return mu, logvar
    
    def reparameterize(self, mu, logvar):
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        return mu + eps * std
    
    def decode(self, z):
        h = F.relu(self.fc_dec1(z))
        h = F.relu(self.fc_dec2(h))
        recon = self.fc_out(h)
        return recon
    
    def forward(self, x):
        mu, logvar = self.encode(x)
        z = self.reparameterize(mu, logvar)
        recon = self.decode(z)
        return recon, mu, logvar

def vae_loss(x, recon, mu, logvar):
    recon_loss = F.mse_loss(recon, x, reduction='sum')
    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())
    return recon_loss + kl_loss

def train_vae(model, dataset, batch_size=1024, lr=1e-3, epochs=10, device=device):
    model.to(device)
    model.train()
    optimizer = optim.Adam(model.parameters(), lr=lr)
    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, pin_memory=True)
    for epoch in tqdm(range(epochs), desc="Training VAE on GPU"):
        total_loss = 0.0
        for batch in loader:
            x = batch[0].to(device)
            optimizer.zero_grad()
            recon, mu, logvar = model(x)
            loss = vae_loss(x, recon, mu, logvar)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        # Optionally print epoch loss:
        # print(f"Epoch {epoch+1}/{epochs}: Loss per sample = {total_loss/len(dataset):.4f}")
    model.to('cpu')
    return model

#####################################
# ENCODE DATA & OUTLIER DETECTION
#####################################
def encode_data_and_find_outliers(model, dataset, outlier_percentile=75):
    """
    Encodes all data using the trained VAE.
    Computes per-sample reconstruction errors.
    Returns:
      - latent_mu: latent representation (mu) for each sample,
      - recon_errors: array of reconstruction errors (MSE per sample),
      - outlier_idx: indices of samples with error > threshold,
      - threshold: the error threshold (e.g., 75th percentile).
    """
    model.eval()
    loader = DataLoader(dataset, batch_size=1024, shuffle=False, pin_memory=True)
    latent_list = []
    error_list = []
    with torch.no_grad():
        for batch in loader:
            x = batch[0]
            recon, mu, logvar = model(x)
            latent_list.append(mu.numpy())
            errors = ((recon.numpy() - x.numpy())**2).mean(axis=1)
            error_list.append(errors)
    latent_mu = np.concatenate(latent_list, axis=0)
    recon_errors = np.concatenate(error_list, axis=0)
    threshold = np.percentile(recon_errors, outlier_percentile)
    # Check for NaNs; if <5% are NaN, drop them; if more, try reprocessing
    nan_mask = np.isnan(latent_mu).any(axis=1)
    frac_nan = np.mean(nan_mask)
    if frac_nan > 0 and frac_nan < 0.05:
        valid_idx = np.where(~nan_mask)[0]
        latent_mu = latent_mu[valid_idx]
    elif frac_nan >= 0.05:
        print("Warning: >5% of latent data is NaN; consider re-running preprocessing.")
    outlier_idx = np.where(recon_errors > threshold)[0]
    return latent_mu, recon_errors, outlier_idx, threshold

#####################################
# ROBUST DP-GMM CLUSTERING FUNCTION
#####################################
def robust_dp_gmm_clustering(latent_mu, outlier_idx, alpha=1.0, max_components=9, max_retries=3):
    """
    Clusters latent representations (excluding outliers) using BayesianGaussianMixture with a Dirichlet Process prior.
    Retries with alternative initialization or increased max_iter if convergence warnings occur.
    Returns:
      - labels_all: cluster assignment for each sample (outliers assigned a special extra cluster)
      - n_clusters: total number of clusters (max up to 9 for inliers, plus one for outliers if present)
      - bgm: the fitted BayesianGaussianMixture model.
    """
    if len(outlier_idx) > 0:
        inlier_mask = np.ones(latent_mu.shape[0], dtype=bool)
        inlier_mask[outlier_idx] = False
        data_cluster = latent_mu[inlier_mask]
    else:
        data_cluster = latent_mu
        inlier_mask = np.ones(latent_mu.shape[0], dtype=bool)
    
    attempts = 0
    while attempts < max_retries:
        try:
            with warnings.catch_warnings(record=True) as w:
                warnings.simplefilter("always")
                bgm = BayesianGaussianMixture(
                    n_components=max_components,
                    weight_concentration_prior=alpha,
                    weight_concentration_prior_type='dirichlet_process',
                    covariance_type='full',
                    init_params='kmeans' if attempts % 2 == 0 else 'random',
                    max_iter=100 * (attempts + 1),
                    random_state=42
                )
                labels_inliers = bgm.fit_predict(data_cluster)
                if any("ConvergenceWarning" in str(warn.message) for warn in w):
                    attempts += 1
                    continue
                break
        except ValueError as e:
            if "NaN" in str(e):
                nan_mask = np.isnan(data_cluster).any(axis=1)
                frac_nan = np.mean(nan_mask)
                if frac_nan < 0.05:
                    data_cluster = data_cluster[~nan_mask]
                    continue
                else:
                    raise e
            else:
                attempts += 1
    else:
        print("Warning: DP-GMM did not converge after several attempts.")
    
    unique_labels = np.unique(labels_inliers)
    label_mapping = {old: new for new, old in enumerate(unique_labels)}
    relabeled_inliers = np.array([label_mapping[l] for l in labels_inliers])
    n_clusters = len(unique_labels)
    labels_all = -1 * np.ones(latent_mu.shape[0], dtype=int)
    labels_all[inlier_mask] = relabeled_inliers
    if len(outlier_idx) > 0:
        labels_all[outlier_idx] = n_clusters  # outlier cluster
        n_clusters += 1
    return labels_all, n_clusters, bgm

#####################################
# MERGE CLUSTERS BY ARTIFACT
#####################################
def merge_clusters_by_artifact(cluster_stats, labels_all):
    """
    Given cluster statistics (dict mapping cluster id -> stats with an 'assigned_artifact' field),
    merge clusters that share the same artifact type.
    Returns new_labels: updated cluster assignments after merging,
            merged_mapping: dict mapping original cluster ids to final artifact id.
    """
    mapping = {}
    merged = {}
    for cid, stats in cluster_stats.items():
        art = stats.get("assigned_artifact", "")
        if art == "":
            continue
        if art in merged:
            mapping[cid] = merged[art]
        else:
            mapping[cid] = cid
            merged[art] = cid
    new_labels = np.array([mapping.get(int(l), int(l)) for l in labels_all])
    return new_labels, merged

#####################################
# CLUSTER ANALYSIS & ARTIFACT LABEL ASSIGNMENT
#####################################
def analyze_clusters_and_label(df, labels_all, feature_cols, artifact_labels=None):
    """
    Computes cluster statistics for each cluster from the latent space.
    Uses a placeholder heuristic: computes z-scores of cluster feature means relative to global means.
    Based on the aggregate z-score, maps each cluster to one of 10 artifact types.
    Updates df's "coarse_class_labels" for detections in classes "G" and "GS".
    Returns a dict mapping cluster id to cluster statistics.
    """
    df["cluster_id"] = labels_all
    clusters = np.unique(labels_all)
    cluster_stats = {}
    
    global_means = df[feature_cols].mean()
    global_stds = df[feature_cols].std() + 1e-8
    artifact_types = [
        "Azimuth Beamforming Error", "Multipath", "Multitarget", "Doppler Artifact",
        "Azimuth Sidelobe", "Monopulse Error", "Elevation-Azimuth Coupling", "NACOM Error",
        "Interference", "Absolute Clutter"
    ]
    # For each cluster, compute mean, variance, and an aggregate z-score.
    for cid in clusters:
        cluster_data = df[df["cluster_id"] == cid]
        size = len(cluster_data)
        if size == 0:
            continue
        feat_means = cluster_data[feature_cols].mean()
        z_scores = (feat_means - global_means) / global_stds
        agg_z = z_scores.abs().sum()
        # Map aggregate z-score to an artifact index between 0 and 9
        artifact_index = int(agg_z / 10) % len(artifact_types)
        assigned_artifact = artifact_types[artifact_index]
        # Compute a dummy probability (for illustration) as inversely proportional to artifact index+1
        artifact_probability = round(100 * (1.0 / (artifact_index + 1)), 2)
        cluster_stats[int(cid)] = {
            "size": int(size),
            "feature_means": feat_means.to_dict(),
            "feature_z_scores": z_scores.to_dict(),
            "aggregate_z_score": float(agg_z),
            "assigned_artifact": assigned_artifact,
            "artifact_probability": artifact_probability
        }
        # Update DF: For each detection, final label = original coarse label prefix ("G" or "GS") + artifact number,
        # except for the outlier cluster (which is labeled as "Absolute Clutter")
        orig_class = df.iloc[0]["labels"]
        prefix = orig_class if orig_class in ["G", "GS"] else ""
        if assigned_artifact == "Absolute Clutter":
            final_label = f"{prefix}_clutter"
        else:
            final_label = f"{prefix}{artifact_index+1}"
        df.loc[df["cluster_id"] == cid, "coarse_class_labels"] = final_label
    return cluster_stats

#####################################
# PROCESS_CLASS FUNCTION: VIB+DP-GMMVAE PIPELINE FOR ONE CLASS ("G" or "GS")
#####################################
def process_class(dfRadar, class_label):
    """
    For detections with coarse_class_labels equal to class_label ("G" or "GS"):
      1. Extract input features (only quality and angle features).
      2. Train VIB+VAE model (shared encoder) on these features in batches on GPU.
      3. Encode the data to obtain latent representations and compute reconstruction errors.
      4. Split the detections into:
           - Reconstructable: those with reconstruction error below the 75th percentile.
           - Non-reconstructable: those with error above threshold → assign as "Absolute Clutter".
      5. Cluster the reconstructable detections with robust DP-GMM (max 9 clusters).
      6. Merge clusters if multiple clusters represent the same artifact (placeholder).
      7. Analyze clusters and assign an artifact label (using statistical analysis).
      8. Update the DataFrame's "coarse_class_labels" for these detections accordingly.
    Returns the updated DataFrame for the given class and the cluster statistics.
    """
    df_class = dfRadar[dfRadar["labels"] == class_label].copy()
    # Use only quality signals and angles (remove kinematic quantities)
    feature_cols = [
        "SNR", "RCS", "aa", "bb", "cc", "dd", "ee", "ff", "gg",
        "hh", "jj", "kk", "Azimuth", "Elevation Angle"
    ]
    X = df_class[feature_cols].values.astype(np.float32)
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    dataset = TensorDataset(torch.tensor(X_scaled, dtype=torch.float32))
    
    # Train VIB+VAE on this class data on GPU (batch-wise)
    latent_dim = 16  # Can be tuned via cross-validation
    model = VIBVAE(input_dim=X_scaled.shape[1], latent_dim=latent_dim, hidden_dim=128)
    model = train_vae(model, dataset, batch_size=1024, lr=1e-3, epochs=20, device=device)
    
    # Encode all data and compute reconstruction errors
    latent_mu, recon_errors, outlier_idx, threshold = encode_data_and_find_outliers(model, dataset, outlier_percentile=75)
    print(f"[{class_label}] Reconstruction error threshold: {threshold:.4f}, outliers: {len(outlier_idx)}")
    
    # Mark non-reconstructable detections as "Absolute Clutter" immediately
    df_class.loc[df_class.index[outlier_idx], "coarse_class_labels"] = f"{class_label}_clutter"
    
    # For the remaining (reconstructable) detections, perform clustering.
    # Adjust latent_mu and indices to include only inliers.
    if len(outlier_idx) > 0:
        inlier_mask = np.ones(latent_mu.shape[0], dtype=bool)
        inlier_mask[outlier_idx] = False
        latent_inliers = latent_mu[inlier_mask]
    else:
        latent_inliers = latent_mu
        inlier_mask = np.ones(latent_mu.shape[0], dtype=bool)
    
    # Cluster latent inliers using robust DP-GMM (max_components=9)
    labels_inliers, n_clusters, bgm_model = robust_dp_gmm_clustering(latent_mu, outlier_idx, alpha=1.0, max_components=9, max_retries=3)
    
    # Update the cluster labels in df_class for inliers only
    df_class.loc[df_class.index, "cluster_id"] = labels_inliers  # This includes outliers as well
    # For inlier detections, assign preliminary label: e.g., for class "G", label "G"+(cluster_id+1)
    prelim_labels = [f"{class_label}{int(l)+1}" if l != -1 else f"{class_label}_clutter" for l in labels_inliers]
    df_class.loc[df_class.index, "coarse_class_labels"] = prelim_labels
    
    # Compute cluster statistics and assign artifact labels using a placeholder statistical analysis
    cluster_stats = analyze_clusters_and_label(df_class, labels_inliers, feature_cols, artifact_labels=None)
    
    # Optionally, merge clusters that map to the same artifact type (not implemented fully here)
    # new_labels, merged_mapping = merge_clusters_by_artifact(cluster_stats, labels_inliers)
    # df_class["coarse_class_labels"] = [f"{class_label}{int(l)+1}" for l in new_labels]
    
    return df_class, cluster_stats

#####################################
# PLOTTING FUNCTION (CUMULATIVE SCATTER PLOT)
#####################################
def save_scatter_plot(df, test_name, output_plot_folder):
    """
    Creates a cumulative scatter plot of all detections (across all cycles).
    Uses "distY" (x-axis) and "distX" (y-axis) with limits: X [-100, 100], Y [-1, 160].
    Detections are colored and marked by their coarse_class_labels.
    The legend text includes the label and detection count with artifact probability in % (2 decimals).
    Legends are off by default.
    Ground truth is overlaid as magenta markers+lines (marker size 1, line width 1).
    Saves the plot as an HTML file.
    """
    # Define fixed styles for "TT" and "SE"
    fixed_styles = {"TT": {"color": "red", "symbol": "circle", "opacity": 1.0},
                    "SE": {"color": "grey", "symbol": "circle", "opacity": 0.3}}
    # Define distinct palettes for "G" and "GS" labels.
    palette_g = ["orange", "darkorange", "tomato", "orangered", "coral",
                 "sandybrown", "goldenrod", "peru", "chocolate"]
    palette_gs = ["blue", "mediumblue", "royalblue", "dodgerblue", "deepskyblue",
                  "cornflowerblue", "skyblue", "steelblue", "slateblue"]
    
    def get_style(label):
        if label in fixed_styles:
            return fixed_styles[label]
        elif label.startswith("G"):
            try:
                idx = int(label[1:]) - 1
            except:
                idx = 0
            return {"color": palette_g[idx % len(palette_g)], "symbol": "square", "opacity": 1.0}
        elif label.startswith("GS"):
            try:
                idx = int(label[2:]) - 1
            except:
                idx = 0
            return {"color": palette_gs[idx % len(palette_gs)], "symbol": "diamond", "opacity": 1.0}
        elif "_clutter" in label:
            return {"color": "black", "symbol": "x", "opacity": 1.0}
        else:
            return {"color": "black", "symbol": "circle", "opacity": 1.0}
    
    unique_labels = np.sort(df["coarse_class_labels"].unique())
    counts = df["coarse_class_labels"].value_counts().to_dict()
    
    traces = []
    for label in unique_labels:
        subset = df[df["coarse_class_labels"] == label]
        style = get_style(label)
        traces.append(go.Scatter(
            x=subset["distY"],
            y=subset["distX"],
            mode='markers',
            marker=dict(size=2, color=style["color"], symbol=style["symbol"], opacity=style["opacity"]),
            name=f"{label} ({counts.get(label, 0)})",
            showlegend=False
        ))
    
    # Ground truth plot: assume df has "distX" and "distY" (merged from gnd.ftr)
    gt_trace = go.Scatter(
        x=df["distY"],
        y=df["distX"],
        mode='markers+lines',
        marker=dict(size=1, color="magenta"),
        line=dict(width=1, color="magenta"),
        name=f"Ground Truth ({len(df)})",
        showlegend=False
    )
    traces.append(gt_trace)
    
    fig = go.Figure(data=traces)
    fig.update_layout(
        title=f"Cumulative Artifact Scatter Plot - {test_name}",
        xaxis=dict(title="distY", range=[-100, 100]),
        yaxis=dict(title="distX", range=[-1, 160]),
        showlegend=False
    )
    output_plot_path = os.path.join(output_plot_folder, f"{test_name}_scatter.html")
    fig.write_html(output_plot_path)
    print(f"Scatter plot saved as {output_plot_path}")

#####################################
# MAIN PROCESSING FUNCTION FOR MULTIPLE TESTCASES
#####################################
def process_testcases(root_folder, gnd_folder, output_radar_folder, output_stats_folder, output_plot_folder):
    """
    Processes multiple test cases.
    
    Arguments:
      - root_folder: path where radar ftr files are stored (files named as testcase_name.ftr).
      - gnd_folder: path where ground truth ftr files are stored (files named as testcase_name_gnd.ftr).
      - output_radar_folder: folder to save updated radar.ftr (named as testcase_name.ftr).
      - output_stats_folder: folder to save cluster statistics JSON (one level up from output_radar_folder).
      - output_plot_folder: folder to save cumulative scatter plots (HTML).
    """
    os.makedirs(output_radar_folder, exist_ok=True)
    os.makedirs(output_stats_folder, exist_ok=True)
    os.makedirs(output_plot_folder, exist_ok=True)
    
    radar_files = [f for f in os.listdir(root_folder) if f.endswith(".ftr")]
    for radar_file in radar_files:
        test_name = os.path.splitext(radar_file)[0]
        print(f"Processing test case: {test_name}")
        test_path = os.path.join(root_folder, radar_file)
        
        # Load data: radar from root_folder, ground truth from gnd_folder.
        dataset, clustering_feature_cols, dfRadar, dfGND = load_and_preprocess_data(test_path, root_folder, gnd_folder)
        
        # Process classes "G" and "GS" separately using VIB+DP-GMMVAE pipeline.
        df_G, cluster_stats_G = process_class(dfRadar, "G")
        df_GS, cluster_stats_GS = process_class(dfRadar, "GS")
        
        # Update original DataFrame (for classes "G" and "GS", new labels in "coarse_class_labels")
        dfRadar.update(df_G)
        dfRadar.update(df_GS)
        
        # Save updated radar.ftr
        output_radar_path = os.path.join(output_radar_folder, f"{test_name}.ftr")
        dfRadar.to_feather(output_radar_path)
        print(f"Updated radar file saved to: {output_radar_path}")
        
        # Combine and save cluster statistics as JSON
        all_cluster_stats = {"G": cluster_stats_G, "GS": cluster_stats_GS}
        output_stats_path = os.path.join(output_stats_folder, f"{test_name}_cluster_stats.json")
        with open(output_stats_path, 'w') as f:
            json.dump(all_cluster_stats, f, indent=2)
        print(f"Cluster statistics saved to: {output_stats_path}")
        
        # Generate cumulative scatter plot for the test case
        save_scatter_plot(dfRadar, test_name, output_plot_folder)
        
        # Clean up GPU memory and variables before next test case
        del dfRadar, dfGND, dataset
        torch.cuda.empty_cache()

#####################################
# MAIN ENTRY POINT
#####################################
if __name__ == '__main__':
    # Define folder paths (adjust these paths as needed)
    root_folder = "./testcases_root"         # radar ftr files (named as testcase_name.ftr)
    gnd_folder = "./gnd_folder"                # ground truth ftr files (named as testcase_name_gnd.ftr)
    output_radar_folder = "./updated_radar"    # folder where updated radar.ftr files will be saved
    output_stats_folder = "./cluster_stats"    # folder where cluster statistics JSON files will be saved
    output_plot_folder = "./plots"             # folder where cumulative scatter plots will be saved
    
    process_testcases(root_folder, gnd_folder, output_radar_folder, output_stats_folder, output_plot_folder)
